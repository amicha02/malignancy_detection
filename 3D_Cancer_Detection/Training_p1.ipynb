{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5719c0f6",
   "metadata": {},
   "source": [
    "This is a high-level roadmap\n",
    "1. Data loading of the <strong>.mhd</strong> and <strong>.raw</strong> file. \n",
    "2. Segmentation (ch13)\n",
    "3. Grouping (ch14) \n",
    "4. Nodule classification (0/1)\n",
    "5. Nodule analysis and diagnosis (Malignant/Benign)\n",
    "\n",
    "\n",
    "As a reminder, we will classify candidates as nodules or non-nodules (we’ll build another classifier to attempt to tell malignant nodules from benign ones in chapter 14). That means we’re going to assign a single, specific label to each sample that we present to the model. In this case, those labels are “nodule” and “non-nodule,” since each sample represents a single candidate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "012afca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd2581b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-17 14:49:59,461 INFO     pid:24901 training:081:initModel Using Apple's M1 chip as a GPU device.\n",
      "2022-10-17 14:49:59,470 INFO     pid:24901 training:129:main Starting LunaTrainingApp, Namespace(num_workers=1, batch_size=32, epochs=1, tb_prefix='p2ch11', comment='dwlpt')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully transfered model to device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-17 14:50:01,677 INFO     pid:24901 dsets:182:__init__ <dsets.LunaDataset object at 0x7f90922cbf70>: 149400 training samples\n",
      "2022-10-17 14:50:01,684 INFO     pid:24901 dsets:182:__init__ <dsets.LunaDataset object at 0x7f9081b814f0>: 16601 validation samples\n",
      "2022-10-17 14:50:01,685 INFO     pid:24901 training:136:main Epoch 1 of 1, 4669/519 batches of size 32*1\n",
      "2022-10-17 14:50:01,704 WARNING  pid:24901 util.util:219:enumerateWithEstimate E1 Training ----/4669, starting\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully initialized models\n",
      "Started batch iteration\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "import training\n",
    "from util.util import importstr\n",
    "from util.logconf import logging\n",
    "\n",
    "log = logging.getLogger('nb') #<1>\n",
    "\n",
    "def run(app, *argv):\n",
    "    argv = list(argv)\n",
    "    argv.insert(0, '--num-workers=1') #<2>\n",
    "    #log.info(\"Running: {}({!r}).main()\".format(app, argv))\n",
    "    x = app.rsplit('.', 1)\n",
    "    app_cls = importstr(*x)\n",
    "   # print(argv)\n",
    "    app_cls(argv).main() \n",
    "   # log.info(\"Finished: {}.{!r}).main()\".format(app, argv)) #<3>\n",
    "\n",
    "#if __name__ == \"__main__\": #<4>\n",
    "run('prepcache.LunaPrepCacheApp')\n",
    "run('training.LunaTrainingApp', '--epochs=1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1a0d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "run('training.LunaTrainingApp', '--epochs=1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8547b243",
   "metadata": {},
   "source": [
    "1. Logging is the process of writing information into log files. Log files contain information about various events that happened in operating system, software, or in communication. (https://docs.python.org/3/howto/logging.html)\n",
    "\n",
    "\n",
    "2. We assume you have a four-core, eight- thread CPU. Change the 4 if needed.\n",
    "3. This is a slightly cleaner call to \\_\\_import\\_\\_\n",
    "4. As we executed script.py directly <strong>\\_\\_name\\_\\_</strong> variable will be <strong>\\_\\_main\\_\\_</strong>.  Thus, you can test whether your script is being run directly or being imported by something else by testing <strong>\\_\\_name\\_\\_</strong>. If script is getting imported by some other module at that time <strong>\\_\\_name\\_\\_</strong> will be module name. if __name__ == “main”: is used to execute some code only if the file was run directly, and not imported.\n",
    "\n",
    "\n",
    "One way to take advantage of being able to invoke our training by either function call or OS-level process is to wrap the function invocations into a Jupyter Notebook so the code can easily be called from either the native CLI or the browser."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc079601",
   "metadata": {},
   "source": [
    "### logging\n",
    "(https://docs.python.org/3/howto/logging.html)<br>\n",
    "\n",
    "\n",
    "The logging module in Python is a ready-to-use and powerful module that is designed to meet the needs of beginners as well as enterprise teams. By default, there are 5 standard levels indicating the severity of events. Each has a corresponding method that can be used to log events at that level of severity.\n",
    "- DEBUG\n",
    "- INFO\n",
    "- WARNING\n",
    "- ERROR\n",
    "- CRITICAL\n",
    "\n",
    "\n",
    "The output shows the severity level before each message along with root, which is the name the logging module gives to its default logger. This format, which shows the level, name, and message separated by a colon (:), is the default output format that can be configured to include things like timestamp, line number, and other details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8cdda0",
   "metadata": {},
   "source": [
    "## \"training.py\" file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51873666",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sys' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 85>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     82\u001b[0m         val_dl \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitValDl()\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;66;03m#<1>\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m     \u001b[43mLunaTrainingApp\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmain()\n",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36mLunaTrainingApp.__init__\u001b[0;34m(self, sys_argv)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, sys_argv\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sys_argv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \u001b[38;5;66;03m#<2>\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m         sys_argv \u001b[38;5;241m=\u001b[39m \u001b[43msys\u001b[49m\u001b[38;5;241m.\u001b[39margv[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m      5\u001b[0m     parser \u001b[38;5;241m=\u001b[39m argparse\u001b[38;5;241m.\u001b[39mArgumentParser()\n\u001b[1;32m      6\u001b[0m     parser\u001b[38;5;241m.\u001b[39madd_argument(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--num-workers\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      7\u001b[0m         help\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNumber of worker processes for background data loading\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      8\u001b[0m         default\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m,\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m     10\u001b[0m     )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sys' is not defined"
     ]
    }
   ],
   "source": [
    "# Used for computeBatchLoss and logMetrics to index into metrics_t/metrics_a\n",
    "\n",
    "METRICS_LABEL_NDX=0\n",
    "METRICS_PRED_NDX=1\n",
    "METRICS_LOSS_NDX=2\n",
    "METRICS_SIZE = 3\n",
    "\n",
    "\n",
    "class LunaTrainingApp:\n",
    "    def __init__(self, sys_argv=None):\n",
    "        if sys_argv is None: #<2>\n",
    "            sys_argv = sys.argv[1:]\n",
    "        parser = argparse.ArgumentParser()\n",
    "        parser.add_argument('--num-workers',\n",
    "            help='Number of worker processes for background data loading',\n",
    "            default=8,\n",
    "            type=int,\n",
    "        )\n",
    "        parser.add_argument('--batch-size',\n",
    "            help='Batch size to use for training',\n",
    "            default=32,\n",
    "            type=int,\n",
    "        )\n",
    "        parser.add_argument('--epochs',\n",
    "            help='Number of epochs to train for',\n",
    "            default=1,\n",
    "            type=int,\n",
    "        )\n",
    "        self.cli_args = parser.parse_args(sys_argv)\n",
    "        self.time_str = datetime.datetime.now().strftime('%Y-%m-%d_%H.%M.%S') #<3>\n",
    "        self.use_mps1 = torch.backends.mps.is_available()\n",
    "        self.use_mps2 = torch.backends.mps.is_built()\n",
    "        self.device = torch.device(\"mps\" if self.use_mps1 and self.usemps2 else \"cpu\") #<6>\n",
    "        self.model = self.initModel()\n",
    "        self.optimizer = self.initOptimizer()\n",
    "        \n",
    "        \n",
    "    def initModel(self):\n",
    "        model = LunaModel()\n",
    "        if self.use_mps1 and self.use_mps2:\n",
    "            log.info(\"Using Apple's M1 chip as a GPU device.\")\n",
    "            model = model.to(self.device) #<4>\n",
    "        return model\n",
    "\n",
    "    def initOptimizer(self):#<5>\n",
    "        return SGD(self.model.parameters(), lr=0.001, momentum=0.99)\n",
    "    \n",
    "    def initTrainDl(self):\n",
    "        train_ds = LunaDataset(\n",
    "            val_stride=10,\n",
    "            isValSet_bool=False,\n",
    "        )\n",
    "\n",
    "        batch_size = self.cli_args.batch_size\n",
    "     #   if self.use_cuda:\n",
    "     #       batch_size *= torch.cuda.device_count()\n",
    "\n",
    "        train_dl = DataLoader(\n",
    "            train_ds,\n",
    "            batch_size=batch_size,\n",
    "            num_workers=self.cli_args.num_workers,\n",
    "            pin_memory=(self.use_mps1 and self.use_mps2) ,\n",
    "        )\n",
    "\n",
    "        return train_dl\n",
    "\n",
    "    def initValDl(self):\n",
    "        val_ds = LunaDataset(\n",
    "            val_stride=10,\n",
    "            isValSet_bool=True,\n",
    "        )\n",
    "\n",
    "        batch_size = self.cli_args.batch_size\n",
    "      #  if self.use_cuda:\n",
    "      #      batch_size *= torch.cuda.device_count()\n",
    "\n",
    "        val_dl = DataLoader(\n",
    "            val_ds,\n",
    "            batch_size=batch_size,\n",
    "            num_workers=self.cli_args.num_workers,\n",
    "            pin_memory= (self.use_mps1 and self.use_mps2),\n",
    "        )\n",
    "\n",
    "        return val_dl\n",
    "\n",
    "\n",
    "    def main(self):\n",
    "        log.info(\"Starting {}, {}\".format(type(self).__name__, self.cli_args))\n",
    "        train_dl = self.initTrainDl()\n",
    "        val_dl = self.initValDl()\n",
    "        \n",
    "        print(\"Successfully initialized models\")\n",
    "        for epoch_ndx in range(1, self.cli_args.epochs + 1):\n",
    "\n",
    "            log.info(\"Epoch {} of {}, {}/{} batches of size {}*{}\".format(\n",
    "                epoch_ndx,\n",
    "                self.cli_args.epochs,\n",
    "                len(train_dl),\n",
    "                len(val_dl),\n",
    "                self.cli_args.batch_size,\n",
    "                1,\n",
    "            ))\n",
    "            \n",
    "            trnMetrics_t = self.doTraining(epoch_ndx, train_dl)\n",
    "            self.logMtrics(epoch_ndx, 'trn', trnMetrics_t)\n",
    "\n",
    "            valMetrics_t = self.doValidation(epoch_ndx, val_dl)\n",
    "            self.logMetrics(epoch_ndx, 'val', valMetrics_t)\n",
    "\n",
    "        if hasattr(self, 'trn_writer'):\n",
    "            self.trn_writer.close()\n",
    "            self.val_writer.close()\n",
    "        \n",
    "        \n",
    "        \n",
    "        #....\n",
    "    def doTraining(self, epoch_ndx, train_dl):\n",
    "        self.model.train()\n",
    "        trnMetrics_g = torch.zeros(\n",
    "            METRICS_SIZE,\n",
    "            len(train_dl.dataset),\n",
    "            device=self.device,\n",
    "        )\n",
    "\n",
    "        batch_iter = enumerateWithEstimate(\n",
    "            train_dl,\n",
    "            \"E{} Training\".format(epoch_ndx),\n",
    "            start_ndx=train_dl.num_workers,\n",
    "        )\n",
    "        print(\"Started batch iteration\")\n",
    "        for batch_ndx, batch_tup in batch_iter:\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            loss_var = self.computeBatchLoss(\n",
    "                batch_ndx,\n",
    "                batch_tup,\n",
    "                train_dl.batch_size,\n",
    "                trnMetrics_g\n",
    "            )\n",
    "            loss_var.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            # # This is for adding the model graph to TensorBoard.\n",
    "            # if epoch_ndx == 1 and batch_ndx == 0:\n",
    "            #     with torch.no_grad():\n",
    "            #         model = LunaModel()\n",
    "            #         self.trn_writer.add_graph(model, batch_tup[0], verbose=True)\n",
    "            #         self.trn_writer.close()\n",
    "        print(\"Finished batch iteration\")\n",
    "        self.totalTrainingSamples_count += len(train_dl.dataset)\n",
    "\n",
    "        return trnMetrics_g.to('cpu')\n",
    "\n",
    "     def doValidation(self, epoch_ndx, val_dl): #<11>\n",
    "        with torch.no_grad():\n",
    "            self.model.eval()\n",
    "            valMetrics_g = torch.zeros(\n",
    "                METRICS_SIZE,\n",
    "                len(val_dl.dataset),\n",
    "                device=self.device,\n",
    "            )\n",
    "\n",
    "            batch_iter = enumerateWithEstimate(\n",
    "                val_dl,\n",
    "                \"E{} Validation \".format(epoch_ndx),\n",
    "                start_ndx=val_dl.num_workers,\n",
    "            )\n",
    "            for batch_ndx, batch_tup in batch_iter:\n",
    "                self.computeBatchLoss(\n",
    "                    batch_ndx, batch_tup, val_dl.batch_size, valMetrics_g)\n",
    "\n",
    "        return valMetrics_g.to('cpu')\n",
    "\n",
    "    def computeBatchLoss(self, batch_ndx, batch_tup, batch_size, metrics_g): #<7>\n",
    "        input_t, label_t, _series_list, _center_list = batch_tup\n",
    "\n",
    "        input_g = input_t.to(self.device, non_blocking=True) #<8>\n",
    "        label_g = label_t.to(self.device, non_blocking=True) #<8>\n",
    "\n",
    "        logits_g, probability_g = self.model(input_g)\n",
    "\n",
    "        loss_func = nn.CrossEntropyLoss(reduction='none') #<9>\n",
    "        loss_g = loss_func(\n",
    "            logits_g,\n",
    "            label_g[:,1],\n",
    "         )\n",
    "        start_ndx = batch_ndx * batch_size\n",
    "        end_ndx = start_ndx + label_t.size(0)\n",
    "\n",
    "        metrics_g[METRICS_LABEL_NDX, start_ndx:end_ndx] = \\\n",
    "            label_g[:,1].detach() #<10>\n",
    "        metrics_g[METRICS_PRED_NDX, start_ndx:end_ndx] = \\\n",
    "            probability_g[:,1].detach() #<10>\n",
    "        metrics_g[METRICS_LOSS_NDX, start_ndx:end_ndx] = \\\n",
    "            loss_g.detach() #<10>\n",
    "\n",
    "        return loss_g.mean()\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "if __name__ == '__main__': #<1>\n",
    "    LunaTrainingApp().main() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c568bf",
   "metadata": {},
   "source": [
    "1. This instantiates the application object and invokes the <strong>main</strong> method. \n",
    "2. If the caller doesn't provide arguments, we get them from the command line.\n",
    "3. The timestamp is used to help identify training runs. The .now method is used of the datetime library. \n",
    "4. Sends model parameters to the GPU.  It’s important to do so beforeconstructing the optimizer, since, otherwise, the optimizer would be left looking atthe CPU-based parameter objects rather than those copied to the GPU.\n",
    "5.  For our optimizer, we’ll use basic stochastic gradient descent (SGD;https://pytorch.org/docs/stable/optim.html#torch.optim.SGD) with momentum. A learning rate of 0.001 and a momentum of 0.9 are pretty safe choices. Empirically, SGD with those values has worked reasonably well for a wide range of projects.\n",
    "6. <strong>self.use_mps1</strong>  ensures that the current MacOS version is at least 12.3+, and <strong>self.use_mps2</strong> ensures that the current current PyTorch installation was built with MPS activated. If both conditions are met, then Apple's M1 chip can be used as a GPU device to train the model on.\n",
    "7. The <strong>computeBatchLoss</strong> function is being used by both training and validation loops. It computes the loss over a batch of samples. In addition, the func- tion also computes and records per-sample information about the output the model is producing.\n",
    "8. Moving the tensors to GPU.\n",
    "9. Setting the reduction argument of the <strong>nn.CrossEntropyLoss</strong> as \"none\" gives you the loss per sample. Therefore the return of the <strong>computeBatchLoss</strong> function is the averaged loss per sample, which is a single value. Getting the loss just per sample allows us to aggregate the individual losses depending on our project and goals. \n",
    "10. Here the per-sample stats are recorded for posterity, by storing the metrics within the metrics_g variable. Detach is used because non of the metrics need to hold on to gradients.\n",
    "11. The  <strong>doValidation</strong> and <strong>doTraining</strong> are different in that validation is read-only, therefore the loss value returned is not used, and the weights are not updated. Using with torch.no_grad() context, we are explicitly informing PyTorch that no gradients need to be computed. After that with statement, self.model.eval() turns off training-time behaviour. Main differences between validation and training functions are:\n",
    "        - No need to update the weights during validation\n",
    "        - No need to use he lass from the computeBatchLoss\n",
    "        - No need to reference the optimizer\n",
    "        - All that's left inside the validation loop is to call computeBatchLoss, and collect the metrics in valMetrics_g variable, which is a side effect of the call.  \n",
    "\n",
    "\n",
    "The application class <strong>LunaTrainingApp</strong> has two functions by mandate; the <strong>\\_\\_init\\_\\_</strong> and <strong>main</strong>. We are parsing arguments in <strong>\\_\\_init\\_\\_</strong>, and that allows us to configure the application separately from invoking it.\n",
    "\n",
    "## Workflow \n",
    "Before we can begin iterating over each batch in our epoch, some initialization work needs to happen, which includes instantiating the model.  \n",
    "\n",
    "<strong></strong>\n",
    "&emsp;<strong>i.</strong> Initialize our model and optimizer. The model is initialized with random weights. <br>\n",
    "&emsp;<strong>ii.</strong> Initialize our <strong>Dataset</strong> and <strong>DataLoader</strong> instances. <br>\n",
    "&emsp;<strong>iii.</strong> Start Training loop. This is when the batch tuple is loaded, the batch is classified, the loss is calculated, the metrics are recorded, and the weights are updated. <br>\n",
    "&emsp;<strong>iv.</strong> In parallel, the validation loop is initiated where the validation set is loaded as a batch tuple, the batches are classified, the loss is calculated, and the metrics are recorded. <br>\n",
    "&emsp;<strong>v.</strong> This process, excluding the i. step is looped over a predefined number of epochs until the model is fully trained.<br>\n",
    "\n",
    "\n",
    "<strong>LunaDataset</strong> will define the randomizedset of samples that will make up our training epoch, and our <strong>DataLoader</strong> instance\n",
    "will perform the work of loading the data out of our dataset and providing it to\n",
    "our application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6793f670",
   "metadata": {},
   "source": [
    "### Batch\n",
    "The batch's dimensions are  (N, C, D, H, W) which are the number of samples, channels per sample, depth, height, and width. Note that since CT scans are single-intensity, our channel dimension\n",
    "is only size 1. The bridging from the CT Scans to PyTorch tensors is done with the <strong>LunaDataset</strong> class. Below is what includes the batch tuple that is fed to the model:\n",
    "- 5D array\n",
    "- List of boolean array that classifies wether it's a tumor or not (T/F)\n",
    "- List of strings that contain series_uid\n",
    "- The candidate location expressed in IRC coordinates. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fb8ed8",
   "metadata": {},
   "source": [
    "## \"model.py\" file\n",
    "Classification models often have a structure that consists of a tail, a backbone (or body), and a head. Next, the backbone of the network typically contains the bulk of the layers, which\n",
    "are usually arranged in series of blocks, which in this case is the <strong>LunaBlock</strong>. We will use a block that consists of two 3 × 3 convolutions, each followed by an activation, with a max-pooling operation at the end of the\n",
    "block. Twenty-seven voxels are fed in, and one comes out.\n",
    "\n",
    "\n",
    "### LunaBlock\n",
    "Stacking convolutional layers allows the final output voxel (or pixel) to be influenced by an input further away than the size of the convolutional kernel suggests. If that output voxel is fed into another 3 × 3 × 3 kernel as one of the edge voxels, then some of the inputs to the first layer will be outside of the 3 × 3 × 3 area of input to the second. The final output of those two stacked layers has an effective receptive field of 5 × 5 × 5. That means that when taken together, the stacked layers act as similar to a single convolu- tional layer with a larger size.  Two stacked 3 × 3 × 3 layers uses fewer parameters than a full 5 × 5 × 5 convolution would (and so is also faster to compute).\n",
    "\n",
    "Below is the implementation of the block:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f612c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LunaBlock(nn.Module):\n",
    "    def __init__(self, in_channels, conv_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv3d(\n",
    "            in_channels, conv_channels, kernel_size=3, padding=1, bias=True,\n",
    "        )\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv3d(\n",
    "            conv_channels, conv_channels, kernel_size=3, padding=1, bias=True,\n",
    "        )\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.maxpool = nn.MaxPool3d(2, 2)\n",
    "\n",
    "    def forward(self, input_batch):\n",
    "        block_out = self.conv1(input_batch)\n",
    "        block_out = self.relu1(block_out)\n",
    "        block_out = self.conv2(block_out)\n",
    "        block_out = self.relu2(block_out)\n",
    "        return self.maxpool(block_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7537f530",
   "metadata": {},
   "source": [
    "(https://proceedings.neurips.cc/paper/2016/file/c8067ad1937f728f51288b3eb986afaa-Paper.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37b3934",
   "metadata": {},
   "source": [
    "### LunaModel (The full model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8291997a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LunaModel(nn.Module):\n",
    "    def __init__(self, in_channels=1, conv_channels=8):\n",
    "        super().__init__()\n",
    "\n",
    "        self.tail_batchnorm = nn.BatchNorm3d(1) #<1>\n",
    "\n",
    "        self.block1 = LunaBlock(in_channels, conv_channels)\n",
    "        self.block2 = LunaBlock(conv_channels, conv_channels * 2)\n",
    "        self.block3 = LunaBlock(conv_channels * 2, conv_channels * 4)\n",
    "        self.block4 = LunaBlock(conv_channels * 4, conv_channels * 8)\n",
    "\n",
    "        self.head_linear = nn.Linear(1152, 2) #<2>\n",
    "        self.head_softmax = nn.Softmax(dim=1)#<2>\n",
    "        self._init_weights()\n",
    "        \n",
    "     # see also https://github.com/pytorch/pytorch/issues/18182\n",
    "    def _init_weights(self): #<4>\n",
    "        for m in self.modules():\n",
    "            if type(m) in {\n",
    "                nn.Linear,\n",
    "                nn.Conv3d,\n",
    "                nn.Conv2d,\n",
    "                nn.ConvTranspose2d,\n",
    "                nn.ConvTranspose3d,\n",
    "            }:\n",
    "                nn.init.kaiming_normal_(\n",
    "                    m.weight.data, a=0, mode='fan_out', nonlinearity='relu',\n",
    "                )\n",
    "                if m.bias is not None:\n",
    "                    fan_in, fan_out = \\\n",
    "                        nn.init._calculate_fan_in_and_fan_out(m.weight.data)\n",
    "                    bound = 1 / math.sqrt(fan_out)\n",
    "                    nn.init.normal_(m.bias, -bound, bound)\n",
    "                    \n",
    "    def forward(self, input_batch):\n",
    "        bn_output = self.tail_batchnorm(input_batch)\n",
    "\n",
    "        block_out = self.block1(bn_output)\n",
    "        block_out = self.block2(block_out)\n",
    "        block_out = self.block3(block_out)\n",
    "        block_out = self.block4(block_out)\n",
    "\n",
    "        conv_flat = block_out.view(\n",
    "            block_out.size(0), #<3>\n",
    "            -1,\n",
    "        )\n",
    "        linear_output = self.head_linear(conv_flat)\n",
    "\n",
    "        return linear_output, self.head_softmax(linear_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8325d054",
   "metadata": {},
   "source": [
    "1. This is the tail. The input is shifted and scaled so that it has a mean of 0 and a standard deviation of 1. This is a normalization technique. \n",
    "2. Our tail is a fully connected layer followed by a call to <strong>nn.Softmax</strong>. Softmax is a useful function for single-label classification tasks and has a few nice proper- ties: it bounds the output between 0 and 1, it’s relatively insensitive to the absolute range of the inputs (only the relative values of the inputs matter), and it allows our model to express the degree of certainty it has in an answer.\n",
    "(https://machinelearningmastery.com/softmax-activation-function-with-python/)\n",
    "3. The view method flattens the data into a batch of 1D vectors, which is what a fully connected layers expect as input. \n",
    "4. The point of creating the <strong> _init_weights</strong> is to make sure the network’s weights are initialized such that intermediate values and gradients become neither unreasonably small nor unreasonably large. This method can be trated as boilerplate, as the exact details aren’t particularly important.\n",
    "\n",
    "\n",
    "Our backbone is four repeated blocks, with the block implementation pulled out into the separate nn.Module subclass. Since each block ends with a 2 × 2 × 2 max-pool operation, after 4 layers we will have decreased the resolution of the image 16 times in each dimension. Our data's dimensions are 32 × 48 × 48 therefore dividing by 16, we'll end up with 2 x 3 x 3 by the end of the backbone.\n",
    "\n",
    "When looking at the <strong>forward</strong> method, its return value is both the raw logits and the softmax-produced probabilities. The reason why we return both, is because the logits are being used to calculate the <strong>nn.CrossEntropyLoss</strong> during trainig, and the probabilities for classification of the samples. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12286779",
   "metadata": {},
   "source": [
    "### Outputting performance metrics\n",
    "\n",
    "\n",
    "Logging results and progress as we go is important, because if the model doesn not converge or goes off the rails, we notice. Noticing it will enable us to stop spending time training that model. The results per epoch are stored in the <strong>trnMetrics_g</strong> and <strong>valMetrics_g</strong> variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdf1e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LunaTrainingApp:\n",
    "#...\n",
    "METRICS_LABEL_NDX=0 #<1>\n",
    "METRICS_PRED_NDX=1\n",
    "METRICS_LOSS_NDX=2\n",
    "METRICS_SIZE = 3\n",
    "\n",
    "    def logMetrics(\n",
    "            self,\n",
    "            epoch_ndx,\n",
    "            mode_str,\n",
    "            metrics_t,\n",
    "            classificationThreshold=0.5,\n",
    "    ):#<2>\n",
    "        self.initTensorboardWriters()\n",
    "        log.info(\"E{} {}\".format(\n",
    "            epoch_ndx,\n",
    "            type(self).__name__,\n",
    "        ))\n",
    "\n",
    "        negLabel_mask = metrics_t[METRICS_LABEL_NDX] <= classificationThreshold #<3>\n",
    "        negPred_mask = metrics_t[METRICS_PRED_NDX] <= classificationThreshold #<3>\n",
    "\n",
    "        posLabel_mask = ~negLabel_mask #<3>\n",
    "        posPred_mask = ~negPred_mask #<3>\n",
    "\n",
    "        neg_count = int(negLabel_mask.sum())\n",
    "        pos_count = int(posLabel_mask.sum())\n",
    "\n",
    "        neg_correct = int((negLabel_mask & negPred_mask).sum())\n",
    "        pos_correct = int((posLabel_mask & posPred_mask).sum())\n",
    "\n",
    "        metrics_dict = {}\n",
    "        metrics_dict['loss/all'] = \\\n",
    "            metrics_t[METRICS_LOSS_NDX].mean()\n",
    "        metrics_dict['loss/neg'] = \\\n",
    "            metrics_t[METRICS_LOSS_NDX, negLabel_mask].mean()\n",
    "        metrics_dict['loss/pos'] = \\\n",
    "            metrics_t[METRICS_LOSS_NDX, posLabel_mask].mean()\n",
    "\n",
    "        metrics_dict['correct/all'] = (pos_correct + neg_correct) \\\n",
    "            / np.float32(metrics_t.shape[1]) * 100\n",
    "        metrics_dict['correct/neg'] = neg_correct / np.float32(neg_count) * 100\n",
    "        metrics_dict['correct/pos'] = pos_correct / np.float32(pos_count) * 100\n",
    "\n",
    "        log.info(\n",
    "            (\"E{} {:8} {loss/all:.4f} loss, \"\n",
    "                 + \"{correct/all:-5.1f}% correct, \"\n",
    "            ).format(\n",
    "                epoch_ndx,\n",
    "                mode_str,\n",
    "                **metrics_dict,\n",
    "            )\n",
    "        )\n",
    "        log.info(\n",
    "            (\"E{} {:8} {loss/neg:.4f} loss, \"\n",
    "                 + \"{correct/neg:-5.1f}% correct ({neg_correct:} of {neg_count:})\"\n",
    "            ).format(\n",
    "                epoch_ndx,\n",
    "                mode_str + '_neg',\n",
    "                neg_correct=neg_correct,\n",
    "                neg_count=neg_count,\n",
    "                **metrics_dict,\n",
    "            )\n",
    "        )\n",
    "        log.info(\n",
    "            (\"E{} {:8} {loss/pos:.4f} loss, \"\n",
    "                 + \"{correct/pos:-5.1f}% correct ({pos_correct:} of {pos_count:})\"\n",
    "            ).format(\n",
    "                epoch_ndx,\n",
    "                mode_str + '_pos',\n",
    "                pos_correct=pos_correct,\n",
    "                pos_count=pos_count,\n",
    "                **metrics_dict,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        writer = getattr(self, mode_str + '_writer')\n",
    "\n",
    "        for key, value in metrics_dict.items():\n",
    "            writer.add_scalar(key, value, self.totalTrainingSamples_count)\n",
    "\n",
    "        writer.add_pr_curve(\n",
    "            'pr',\n",
    "            metrics_t[METRICS_LABEL_NDX],\n",
    "            metrics_t[METRICS_PRED_NDX],\n",
    "            self.totalTrainingSamples_count,\n",
    "        )\n",
    "\n",
    "        bins = [x/50.0 for x in range(51)]\n",
    "\n",
    "        negHist_mask = negLabel_mask & (metrics_t[METRICS_PRED_NDX] > 0.01) #<2>\n",
    "        posHist_mask = posLabel_mask & (metrics_t[METRICS_PRED_NDX] < 0.99) #<2>\n",
    "\n",
    "        if negHist_mask.any():\n",
    "            writer.add_histogram(\n",
    "                'is_neg',\n",
    "                metrics_t[METRICS_PRED_NDX, negHist_mask],\n",
    "                self.totalTrainingSamples_count,\n",
    "                bins=bins,\n",
    "            )\n",
    "        if posHist_mask.any():\n",
    "            writer.add_histogram(\n",
    "                'is_pos',\n",
    "                metrics_t[METRICS_PRED_NDX, posHist_mask],\n",
    "                self.totalTrainingSamples_count,\n",
    "                bins=bins,\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff922c0",
   "metadata": {},
   "source": [
    "1. These named array indexes are declared at module-level scope. This is to gain access of the label,prediction, and loss for each and every training and validation. \n",
    "2. The epoch_ndx is used for display when logging the results. The mode_str holds information about wether the metrics are for training or validation. \n",
    "3. In the main function we consume either <strong>trnMetrics_t</strong> or <strong>valMetrics_t</strong>, and is passed as the <strong>metrics_t</strong> parameter. Here we construct masks that will let us limit our metrics to only the nod- ule or non-nodule samples. Our nodule status labels are simply True or False, therefore here we get an array of binary values where a True value corresponds to a non-nodule (aka negative) label for the sample in question. The positive masks are simply the inverse of the negative masks.\n",
    "\n",
    "First we compute the average loss over the entire epoch. Since the loss is the single metric that is being minimized during training, we always want to be able to keep track of it. Then we limit the loss averaging to only those samples with a negative label using the negLabel_mask we just made. We do the same with the positive loss. Com- puting a per-class loss like this can be useful if one class is persistently harder to classify than another, since that knowledge can help drive investigation and improvements.\n",
    "We’ll close out the calculations with determining the fraction of samples we classi- fied correctly, as well as the fraction correct from each label. Since we will display these numbers as percentages in a moment, we also multiply the values by 100. Similar to the loss, we can use these numbers to help guide our efforts when making improve- ments. After the calculations, we then log our results with three calls to log.info.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a758142f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
