{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4225bcba",
   "metadata": {},
   "source": [
    "# Foundational model and training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5719c0f6",
   "metadata": {},
   "source": [
    "This is a high-level roadmap\n",
    "1. Data loading of the <strong>.mhd</strong> and <strong>.raw</strong> file. \n",
    "2. Segmentation (ch13)\n",
    "3. Grouping (ch14) \n",
    "4. Nodule classification (0/1)\n",
    "5. Nodule analysis and diagnosis (Malignant/Benign)\n",
    "\n",
    "\n",
    "As a reminder, we will classify candidates as nodules or non-nodules (we’ll build another classifier to attempt to tell malignant nodules from benign ones in chapter 14). That means we’re going to assign a single, specific label to each sample that we present to the model. In this case, those labels are “nodule” and “non-nodule,” since each sample represents a single candidate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "012afca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abd2581b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-07 01:51:01,202 INFO     pid:16046 nb:012:run Running: training.LunaTrainingApp(['--num-workers=4', '--epochs= 1']).main()\n",
      "2022-10-07 01:51:01,224 INFO     pid:16046 training:065:initModel Using Apple's M1 chip as a GPU device.\n",
      "2022-10-07 01:51:01,257 INFO     pid:16046 training:075:main Starting LunaTrainingApp, Namespace(num_workers=4, epochs=1)\n",
      "2022-10-07 01:51:01,257 INFO     pid:16046 nb:015:run Finished: training.LunaTrainingApp.['--num-workers=4', '--epochs= 1']).main()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "import training\n",
    "from util.util import importstr\n",
    "from util.logconf import logging\n",
    "\n",
    "log = logging.getLogger('nb') #<1>\n",
    "\n",
    "def run(app, *argv):\n",
    "    argv = list(argv)\n",
    "    argv.insert(0, '--num-workers=4') #<2>\n",
    "    log.info(\"Running: {}({!r}).main()\".format(app, argv))\n",
    "    app_cls = importstr(*app.rsplit('.', 1))\n",
    "    app_cls(argv).main()\n",
    "    log.info(\"Finished: {}.{!r}).main()\".format(app, argv)) #<3>\n",
    "run('training.LunaTrainingApp', '--epochs= 1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8547b243",
   "metadata": {},
   "source": [
    "1. Logging is the process of writing information into log files. Log files contain information about various events that happened in operating system, software, or in communication. (https://docs.python.org/3/howto/logging.html)\n",
    "\n",
    "\n",
    "2. We assume you have a four-core, eight- thread CPU. Change the 4 if needed.\n",
    "3. This is a slightly cleaner call to \\_\\_import\\_\\_\n",
    "\n",
    "One way to take advantage of being able to invoke our training by either function call or OS-level process is to wrap the function invocations into a Jupyter Notebook so the code can easily be called from either the native CLI or the browser."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc079601",
   "metadata": {},
   "source": [
    "### logging\n",
    "(https://docs.python.org/3/howto/logging.html)<br>\n",
    "\n",
    "\n",
    "The logging module in Python is a ready-to-use and powerful module that is designed to meet the needs of beginners as well as enterprise teams. By default, there are 5 standard levels indicating the severity of events. Each has a corresponding method that can be used to log events at that level of severity.\n",
    "- DEBUG\n",
    "- INFO\n",
    "- WARNING\n",
    "- ERROR\n",
    "- CRITICAL\n",
    "\n",
    "\n",
    "The output shows the severity level before each message along with root, which is the name the logging module gives to its default logger. This format, which shows the level, name, and message separated by a colon (:), is the default output format that can be configured to include things like timestamp, line number, and other details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8cdda0",
   "metadata": {},
   "source": [
    "## \"training.py\" file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51873666",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LunaTrainingApp:\n",
    "    def __init__(self, sys_argv=None):\n",
    "        if sys_argv is None: #<2>\n",
    "            sys_argv = sys.argv[1:]\n",
    "\n",
    "        parser = argparse.ArgumentParser()\n",
    "        parser.add_argument('--num-workers',\n",
    "            help='Number of worker processes for background data loading',\n",
    "            default=8,\n",
    "            type=int,\n",
    "        )\n",
    "       # parser.add_argument('--epochs',\n",
    "       #     help='Number of epochs to train for',\n",
    "       #     default=1,\n",
    "       #     type=int,\n",
    "       # )\n",
    "\n",
    "        self.cli_args = parser.parse_args(sys_argv)\n",
    "        self.time_str = datetime.datetime.now().strftime('%Y-%m-%d_%H.%M.%S') #<3>\n",
    "        self.use_mps1 = torch.backends.mps.is_available()\n",
    "        self.use_mps2 = torch.backends.mps.is_built()\n",
    "        self.device = torch.device(\"mps\" if self.use_mps1 and self.usemps2 else \"cpu\")\n",
    "\n",
    "        self.model = self.initModel()\n",
    "        self.optimizer = self.initOptimizer()\n",
    "        \n",
    "        \n",
    "    def initModel(self):\n",
    "        model = LunaModel()\n",
    "        if self.use_mps1 and self.use_mps2:\n",
    "            log.info(\"Using Apple's M1 chip as a GPU device.\")\n",
    "            model = model.to(self.device) #<4>\n",
    "        return model\n",
    "\n",
    "    def initOptimizer(self):#<5>\n",
    "        return SGD(self.model.parameters(), lr=0.001, momentum=0.99)\n",
    "\n",
    "\n",
    "    def main(self):\n",
    "        log.info(\"Starting {}, {}\".format(type(self).__name__, self.cli_args))\n",
    "if __name__ == '__main__': #<1>\n",
    "    LunaTrainingApp().main() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c568bf",
   "metadata": {},
   "source": [
    "1. This instantiates the application object and invokes the <strong>main</strong> method. \n",
    "2. If the caller doesn't provide arguments, we get them from the command line.\n",
    "3. The timestamp is used to help identify training runs. The .now method is used of the datetime library. \n",
    "4. Sends model parameters to the GPU.  It’s important to do so beforeconstructing the optimizer, since, otherwise, the optimizer would be left looking atthe CPU-based parameter objects rather than those copied to the GPU.\n",
    "5.  For our optimizer, we’ll use basic stochastic gradient descent (SGD;https://pytorch.org/docs/stable/optim.html#torch.optim.SGD) with momentum. A learning rate of 0.001 and a momentum of 0.9 are pretty safe choices. Empirically, SGD with those values has worked reasonably well for a wide range of projects. \n",
    "\n",
    "\n",
    "The application class <strong>LunaTrainingApp</strong> has two functions by mandate; the <strong>\\_\\_init\\_\\_</strong> and <strong>main</strong>. We are parsing arguments in <strong>\\_\\_init\\_\\_</strong>, and that allows us to configure the application separately from invoking it.\n",
    "\n",
    "## Workflow \n",
    "Before we can begin iterating over each batch in our epoch, some initialization work needs to happen, which includes instantiating the model.  \n",
    "\n",
    "<strong></strong>\n",
    "&emsp;<strong>i.</strong> Initialize our model and optimizer. The model is initialized with random weights. <br>\n",
    "&emsp;<strong>ii.</strong> Initialize our <strong>Dataset</strong> and <strong>DataLoader</strong> instances. <br>\n",
    "&emsp;<strong>iii.</strong> Start Training loop. This is when the batch tuple is loaded, the batch is classified, &emsp;&emsp;the loss is calculated, the metrics are recorded, and the weights are updated. <br>\n",
    "&emsp;<strong>iv.</strong> In parallel, the validation loop is initiated where the validation set is loaded as &emsp;&emsp;a batch tuple, the batches are classified, the loss is calculated, and the metrics &emsp;&emsp;are recorded. <br>\n",
    "&emsp;<strong>v.</strong> This process, excluding the i. step is looped over a predefined number of epochs &emsp;&emsp;until the model is fully trained.<br>\n",
    "\n",
    "\n",
    "<strong>LunaDataset</strong> will define the randomizedset of samples that will make up our training epoch, and our <strong>DataLoader</strong> instance\n",
    "will perform the work of loading the data out of our dataset and providing it to\n",
    "our application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de5dadc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
