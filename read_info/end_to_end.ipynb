{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bridging CT segmentation and nodule candidate classification\n",
    "## NoduleAnalysis App"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NoduleAnalysisApp.main()\n",
    "The next step is to convert the segmentation output into sample tuples. Grouping the data involves finding the dashed outline around what;s flagged by the model, and then taking the segmentation output, which is the voxels flagged by the segmentation model, and coordfinate the center of each lump of flagged voxels as index, row and column.  \n",
    "#### LunaDataset (val stride=10, and isvallset = true)\n",
    "First time the LunaDataset is called by running the nodule_analysis.py script, there is no values inside the candidateInfo_list, hence why we extract the values while using cache. \n",
    "1. If it's the validation set, then extract every Xth (val_stride) value from the series set, otherwise delete the validation entries and that's your training set. The splitting is at the CT/series_uid level. __getitem__ atribute.  When accessing an it gives batch_size * 48x48 matrices. \n",
    "2. For nodule candidatw generation, we iterate over all the series_set to extract their corresponding CT. Each object includes the 121x512x512 CT scan. \n",
    "3. <strong>segmentCt</strong> <br>\n",
    "This line of code runs the segmentation model on the CT scan.\n",
    "    - <strong>a.</strong> seg_dl<br> \n",
    "    Every CT scan has 121 slices the index direction. When accessing an index of the Segmentation dataset, we are given a 7x512x512 CT scan, a posiive mask of dimensions 1x512x512, the series_uid and the slice index.\n",
    "    -  <strong>b.</strong><br> The segmentation model takes batch_size x 7 context slices x 512 x 512 as input, which means in terms of width and length covers the entire area of the CT scan. The output is batch_size x 1 x 512 x 512. The prediction is a boolean mask\n",
    "    Note that during training, the last two dimensions of the input if 64 by 64, representing just voxels. \n",
    "    -  <strong>c.</strong><br> Prior to this, we've been able to create the boolean mask for all channels accross the CT scan. Here we apply binary erosion which is a mathematical morhology structuring element for shrinking the shapes in an image. \n",
    "\n",
    "4. <strong>groupSegmentationOutput</strong> <br>\n",
    "\n",
    "- a. The label function will take nonzero pixels and mark them as belonging to the same group. The second part of the output is the number of objects found.\n",
    "- b. Gets the center mass coordinates for each group in the IRC coordinates. This function is intented for center mass calculation, which is why we offset the array by adding 1001 to all pixel density values. \n",
    "- c. Here we build our candidate info tuple and append it to the list of detections. \n",
    "5. <strong>Nodule Classification</strong> <br>\n",
    "The classification model resides in the classifications directory. The LunaModel is trained on the LunaData. The LunaData are basically 32x48x48 data that are being classified 0(non-nodule) or 1 (nodule). A sample that is not a nodule has [1, 0] as a label. \n",
    "- a. Column 1 of the probability predictions is the predicted probability that should be kept. \n",
    "- b. We are zipping the IRC center coordinates, nodule probability, malignancy probability, and also we get the center coordinates in XYZ representation. \n",
    "6. If we have the ground truth data which resides in the candidateInfo_dict, compute and pring the confusion matrix and add the results to the total.\n",
    "- a. The Complete Miss column is basicaly Nodules that were completely missed, wether it they were Benign or Malignant. Anythings that makes it through the predicted nodules, are the ones that survived the nodule classifier. Any non-nodules under that column are basically False Positive (falsely classified as nodules). The number of Non-nodules that were filtered out were the ones that didn't even make it to the malignancy model, as they correctly had been classified as non-nodules.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "nant']\n",
    "\n",
    "    if do_mal:\n",
    "        col_labels = ['', 'Complete Miss', 'Filtered Out', 'Pred. Benign', 'Pred. Malignant']\n",
    "    else:\n",
    "        col_laies UID to use.\",\n",
    "        )\n",
    "\n",
    "        self.cli_args = parser.parse_args(sys_arg\n",
    "            log.debug(self.cli_args.classification_path)\n",
    "            cls_model = LunaModel()\n",
    "            cls_dict = torch.load(self.cli_args.classification_path, map_location=torch.device('mps'))     \n",
    "  \n",
    "            cls_model.load_state_dict(cls_dict['model_state'])\n",
    "            cls_model.eval()\n",
    "           # cls_model.to(self.device)\n",
    "        else:\n",
    "            cls_model = None\n",
    "\n",
    "        if self.cli_args.malignancy_path:\n",
    "            malignancy_model = LunaModel()\n",
    "            malignancy_dict = torch.load(self.cli_args.malignancy_path)\n",
    "            malignancy_model.load_state_dict(malignancy_dict['model_state'])\n",
    "            malignancy_model.eval()\n",
    "            if self.use_cuda:\n",
    "                malignancy_model.to(self.device)\n",
    "        else:\n",
    "            malignancy_model = None\n",
    "\n",
    "\n",
    "        return seg_model, cls_model, malignancy_model\n",
    "\n",
    "    def initModelPath(self, type_str):\n",
    "        local_path = os.path.join(\n",
    "            'models',\n",
    "            type_str + '_{}_{}.{}.state'.format('*', '*', 'best'),\n",
    "        )\n",
    "\n",
    "        file_list = glob.glob(local_path)\n",
    "        if not file_list:\n",
    "            pretrained_path = os.path.join(\n",
    "\n",
    "                'models',\n",
    "                type_str + '_{}_{}.{}.state'.format('*', '*', '*'),\n",
    "            )\n",
    "            file_list = glob.glob(pretrained_path)\n",
    "        else:\n",
    "            pretrained_path = None\n",
    "\n",
    "        file_list.sort()\n",
    "\n",
    "        try:\n",
    "            return file_list[-1]\n",
    "        except IndexError:\n",
    "            log.debug([local_path, pretrained_path, file_list])\n",
    "            raise\n",
    "    \n",
    "\n",
    "    \n",
    "    def initSegmentationDl(self, series_uid):\n",
    "        seg_ds = Luna2dSegmentationDataset(\n",
    "                contextSlices_count=3,\n",
    "                series_uid=series_uid,\n",
    "                fullCt_bool=True,\n",
    "            )\n",
    "        seg_dl = DataLoader(\n",
    "            seg_ds,\n",
    "            batch_size=self.cli_args.batch_size ,\n",
    "            num_workers=self.cli_args.num_workers,\n",
    "            pin_memory=0,\n",
    "        )\n",
    "\n",
    "        return seg_dl\n",
    "    \n",
    "\n",
    "    def main(self):\n",
    "        log.info(\"Starting {}, {}\".format(type(self).__name__, self.cli_args))\n",
    "        val_ds = LunaDataset(\n",
    "            val_stride=10,\n",
    "            isValSet_bool=True,\n",
    "        )  #<1>\n",
    "        val_set = set(\n",
    "            candidateInfo_tup.series_uid\n",
    "            for candidateInfo_tup in val_ds.candidateInfo_list\n",
    "        )\n",
    "        positive_set = set(\n",
    "            candidateInfo_tup.series_uid\n",
    "            for candidateInfo_tup in getCandidateInfoList()\n",
    "            if candidateInfo_tup.isNodule_bool\n",
    "        )\n",
    "\n",
    "        if self.cli_args.series_uid:\n",
    "            series_set = set(self.cli_args.series_uid.split(','))\n",
    "        else:\n",
    "            series_set = set(\n",
    "                candidateInfo_tup.series_uid\n",
    "                for candidateInfo_tup in getCandidateInfoList()\n",
    "            )\n",
    "\n",
    "        if self.cli_args.include_train:\n",
    "            train_list = sorted(series_set - val_set)\n",
    "        else:\n",
    "            train_list = []\n",
    "        val_list = sorted(series_set & val_set)\n",
    "      \n",
    "\n",
    "        candidateInfo_dict = getCandidateInfoDict()\n",
    "        series_iter = enumerateWithEstimate(\n",
    "            val_list + train_list,\n",
    "            \"Series\",\n",
    "        )\n",
    "\n",
    "        all_confusion = np.zeros((3, 4), dtype=np.int)\n",
    "\n",
    "        for _, series_uid in series_iter:\n",
    "            ct = getCt(series_uid) #<2>\n",
    "            mask_a = self.segmentCt(ct, series_uid) #<3>\n",
    "            candidateInfo_list = self.groupSegmentationOutput(\n",
    "                series_uid, ct, mask_a) #<4>\n",
    "           \n",
    "            classifications_list = self.classifyCandidates(\n",
    "                ct, candidateInfo_list) #<5>\n",
    "            if not self.cli_args.run_validation:\n",
    "                print(f\"found nodule candidates in {series_uid}:\")\n",
    "                for prob, prob_mal, center_xyz, center_irc in classifications_list:\n",
    "                    if prob > 0.5: \n",
    "                        s = f\"nodule prob {prob:.3f}, \"\n",
    "                        if self.malignancy_model:\n",
    "                            s += f\"malignancy prob {prob_mal:.3f}, \"\n",
    "                        s += f\"center xyz {center_xyz}\"\n",
    "                        print(s)\n",
    "            if series_uid in candidateInfo_dict: #<6>\n",
    "                one_confusion = match_and_score(\n",
    "                    classifications_list, candidateInfo_dict[series_uid]\n",
    "                )\n",
    "                print(one_confusion)\n",
    "                all_confusion += one_confusion\n",
    "                print_confusion(series_uid, one_confusion, self.malignancy_model is not None) #<a>\n",
    "                print_confusion(\"Total\", all_confusion, self.malignancy_model is not None)\n",
    "      \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def initClassificationDl(self, candidateInfo_list):\n",
    "        cls_ds = LunaDataset(\n",
    "                sortby_str='series_uid',\n",
    "                candidateInfo_list=candidateInfo_list,\n",
    "            )\n",
    "        cls_dl = DataLoader(\n",
    "            cls_ds,\n",
    "            batch_size=self.cli_args.batch_size * (torch.cuda.device_count() if self.use_cuda else 1),\n",
    "            num_workers=self.cli_args.num_workers,\n",
    "            pin_memory=self.use_cuda,\n",
    "        )\n",
    "        return cls_dl\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def classifyCandidates(self, ct, candidateInfo_list):\n",
    "        cls_dl = self.initClassificationDl(candidateInfo_list)\n",
    "        classifications_list = []\n",
    "        for batch_ndx, batch_tup in enumerate(cls_dl):\n",
    "            input_t, _, _, series_list, center_list = batch_tup\n",
    "\n",
    "            #input_g = input_t.to(self.device)\n",
    "            with torch.no_grad():\n",
    "                _, probability_nodule_g = self.cls_model(input_t)\n",
    "                if self.malignancy_model is not None:\n",
    "                    _, probability_mal_g = self.malignancy_model(input_t)\n",
    "                else:\n",
    "                    probability_mal_g = torch.zeros_like(probability_nodule_g)\n",
    "\n",
    "            zip_iter = zip(center_list,\n",
    "                probability_nodule_g[:,1].tolist(),\n",
    "                probability_mal_g[:,1].tolist())\n",
    "            for center_irc, prob_nodule, prob_mal in zip_iter:\n",
    "                center_xyz = irc2xyz(center_irc,\n",
    "                    direction_a=ct.direction_a,\n",
    "                    origin_xyz=ct.origin_xyz,\n",
    "                    vxSize_xyz=ct.vxSize_xyz,\n",
    "                )\n",
    "                cls_tup = (prob_nodule, prob_mal, center_xyz, center_irc)\n",
    "                classifications_list.append(cls_tup) #<b>\n",
    "        return classifications_list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "    def groupSegmentationOutput(self, series_uid,  ct, clean_a):\n",
    "        candidateLabel_a, candidate_count = measurements.label(clean_a) #<a>\n",
    "        centerIrc_list = measurements.center_of_mass(\n",
    "            ct.hu_a.clip(-1000, 1000) + 1001,\n",
    "            labels=candidateLabel_a,\n",
    "            index=np.arange(1, candidate_count+1),\n",
    "        )  #<b>\n",
    "\n",
    "        candidateInfo_list = []\n",
    "        for i, center_irc in enumerate(centerIrc_list):\n",
    "            center_xyz = irc2xyz(\n",
    "                center_irc,\n",
    "                ct.origin_xyz,\n",
    "                ct.vxSize_xyz,\n",
    "                ct.direction_a,\n",
    "            )\n",
    "            assert np.all(np.isfinite(center_irc)), repr(['irc', center_irc, i, candidate_count])\n",
    "            assert np.all(np.isfinite(center_xyz)), repr(['xyz', center_xyz])\n",
    "            candidateInfo_tup = \\\n",
    "                CandidateInfoTuple(False, False, False, 0.0, series_uid, center_xyz) #<c> \n",
    "            candidateInfo_list.append(candidateInfo_tup)\n",
    "        \n",
    "        return candidateInfo_list\n",
    "    \n",
    "\n",
    "\n",
    "    def segmentCt(self, ct, series_uid):\n",
    "        with torch.no_grad():\n",
    "            output_a = np.zeros_like(ct.hu_a, dtype=np.float32)\n",
    "            seg_dl = self.initSegmentationDl(series_uid)  #  <a>\n",
    "            for input_t, _, _, slice_ndx_list in seg_dl:\n",
    "                input_g = input_t.to(self.device)\n",
    "                prediction_g = self.seg_model(input_g) #<b>\n",
    "                for i, slice_ndx in enumerate(slice_ndx_list):\n",
    "                    output_a[slice_ndx] = prediction_g[i].cpu().numpy()\n",
    "\n",
    "            mask_a = output_a > 0.5\n",
    "            mask_a = morphology.binary_erosion(mask_a, iterations=1) #<c>\n",
    "        return mask_a\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    NoduleAnalysisApp().main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
