{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "854af2a6",
   "metadata": {},
   "source": [
    "# Convolutions in action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd215df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3339cb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "bb4a5682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import torch.optim as optim\n",
    "import datetime\n",
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "data_path = '../data-unversioned/p1ch6/'\n",
    "cifar10 = datasets.CIFAR10(\n",
    "    data_path, train=True, download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
    "                            (0.2470, 0.2435, 0.2616))\n",
    "    ]))\n",
    "cifar10_val = datasets.CIFAR10(\n",
    "    data_path, train=False, download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
    "                            (0.2470, 0.2435, 0.2616))\n",
    "    ]))\n",
    "label_map = {0: 0, 2: 1}\n",
    "class_names = ['airplane', 'bird']\n",
    "cifar2 = [(img, label_map[label])\n",
    "          for img, label in cifar10\n",
    "          if label in [0, 2]]\n",
    "cifar2_val = [(img, label_map[label])\n",
    "              for img, label in cifar10_val\n",
    "              if label in [0, 2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c890dc8",
   "metadata": {},
   "source": [
    "Here we have 3 input features per pixel (the RGB channels) and an arbitrary number of channels in the output (16). We need the channels to be able to detect many different types of features. Also, because we are randomly initializing them, some of the features we’ll get, even after training, will turn out to be useless.2 Let’s stick to a kernel size of 3 × 3. A 2D convolution pass produces a 2D image as output, whose pixels are a weighted sum over neighborhoods of the input image.\n",
    "\n",
    "In our case, both the kernel weights and the bias conv.weight are initialized randomly, so the output image will not be particu- larly meaningful. As usual, we need to add the zeroth batch dimension with unsqueeze if we want to call the conv module with one input image, since nn.Conv2d expects a Batches × Channels × Height × Width shaped tensor as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5c9fd7f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of kernel weights and bias\n",
      "torch.Size([16, 3, 3, 3]) torch.Size([16])\n",
      "\n",
      "Shape of input and output\n",
      "torch.Size([1, 3, 32, 32]) torch.Size([1, 16, 30, 30])\n"
     ]
    }
   ],
   "source": [
    "conv = nn.Conv2d(3, 16, kernel_size=3)\n",
    "print('Shape of kernel weights and bias')\n",
    "print(conv.weight.shape, conv.bias.shape)\n",
    "print('')\n",
    "img, _ = cifar2[0]\n",
    "output = conv(img.unsqueeze(0))\n",
    "print('Shape of input and output')\n",
    "print(img.unsqueeze(0).shape, output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "10f3430f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x345.6 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 4.8))  # bookskip\n",
    "ax1 = plt.subplot(1, 2, 1)   # bookskip\n",
    "plt.title('output')   # bookskip\n",
    "plt.imshow(output[0, 0].detach(), cmap='gray')\n",
    "plt.subplot(1, 2, 2, sharex=ax1, sharey=ax1)  # bookskip\n",
    "plt.imshow(img.mean(0), cmap='gray')  # bookskip\n",
    "plt.title('input')  # bookskip\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39da10a0",
   "metadata": {},
   "source": [
    "#### Kernel 1\n",
    "Each pixel in the output gets the mean of its neighbor (3x3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fb4f5602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 32, 32]) torch.Size([1, 1, 32, 32])\n",
      "Output after appplying the Kernel that averages based on neighborhood pixels\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "conv = nn.Conv2d(3, 1, kernel_size=3, padding=1)\n",
    "output = conv(img.unsqueeze(0))\n",
    "print(img.unsqueeze(0).shape, output.shape)\n",
    "with torch.no_grad():\n",
    "    conv.bias.zero_()\n",
    "with torch.no_grad():\n",
    "    conv.weight.fill_(1.0 / 9.0)\n",
    "    \n",
    "print('Output after appplying the Kernel that averages based on neighborhood pixels')\n",
    "output = conv(img.unsqueeze(0))\n",
    "plt.imshow(output[0, 0].detach(), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021d2250",
   "metadata": {},
   "source": [
    "#### Kernel 2\n",
    "convolution kernel enhances the vertical edges. When an edge is \"detected\" that means that the output has a high magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6e0d73b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = nn.Conv2d(3, 1, kernel_size=3, padding=1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    conv.weight[:] = torch.tensor([[-1.0, 0.0, 1.0],\n",
    "                                   [-1.0, 0.0, 1.0],\n",
    "                                   [-1.0, 0.0, 1.0]])\n",
    "    conv.bias.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "76bcc518",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x345.6 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "output = conv(img.unsqueeze(0))\n",
    "plt.figure(figsize=(10, 4.8))  # bookskip\n",
    "ax1 = plt.subplot(1, 2, 1)   # bookskip\n",
    "plt.title('output')   # bookskip\n",
    "plt.imshow(output[0, 0].detach(), cmap='gray')\n",
    "plt.subplot(1, 2, 2, sharex=ax1, sharey=ax1)  # bookskip\n",
    "plt.imshow(img.mean(0), cmap='gray')  # bookskip\n",
    "plt.title('input')  # bookskip\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d816fd",
   "metadata": {},
   "source": [
    "#### Downsampling technique example\n",
    "Downsample our image by half by using the max pooling technique, we’ll want to use a size of 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "96f52404",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 32, 32]), torch.Size([1, 3, 16, 16]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool = nn.MaxPool2d(2)\n",
    "output = pool(img.unsqueeze(0))\n",
    "img.unsqueeze(0).shape, output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8468345b",
   "metadata": {},
   "source": [
    "#### NN structure\n",
    "1. The 1st convolution takes us from 3 RGB channels to 16. These 16 independent features are being used to discriminate low-level features of birds and airplanes\n",
    "2. The tanh activation function and yield a resulting 16-channel 32x32 image\n",
    "3. Applying MaxPool2d(2) module shrinks the output to a 16-channel 16x16 image.\n",
    "4. The image is convolved and yields an 8 channel 16 x 16 output which will consist of higher level features\n",
    "5. The tanh activation function and yield a resulting 8-channel 16x16 image\n",
    "6. Applying MaxPool2d(2) module shrinks the output to a 8-channel 8x8 image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a6381e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, padding=1), #1\n",
    "            nn.Tanh(), #2\n",
    "            nn.MaxPool2d(2), #3\n",
    "            nn.Conv2d(16, 8, kernel_size=3, padding=1), #4\n",
    "            nn.Tanh(), #5\n",
    "            nn.MaxPool2d(2),#6\n",
    "            # ... <1> ERROR\n",
    "            nn.Linear(8 * 8 * 8, 32),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(32, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7e363325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters:\n",
      "18090\n",
      "Number of parameters per layer:\n",
      "[432, 16, 1152, 8, 16384, 32, 64, 2]\n"
     ]
    }
   ],
   "source": [
    "numel_list = [p.numel() for p in model.parameters()]\n",
    "print('Total number of parameters:' + '\\n' + str(sum(numel_list)))\n",
    "print('Number of parameters per layer:' + '\\n' + str(numel_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3a83ec",
   "metadata": {},
   "source": [
    "### Writing our network as a nn.Module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81adfdbc",
   "metadata": {},
   "source": [
    "A forward function is defined here, that takes the inputs to the module and returns the output. This is where we define our module’s computation. Writing the forward function explicitly allows us to manipulate the outputs between layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5876e5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1) \n",
    "        self.act1 = nn.Tanh()\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1) \n",
    "        self.act2 = nn.Tanh()\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.fc1 = nn.Linear(8 * 8 * 8, 32)\n",
    "        self.act3 = nn.Tanh()\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "    def forward(self, x):\n",
    "        out = self.pool1(self.act1(self.conv1(x)))\n",
    "        out = self.pool2(self.act2(self.conv2(out)))\n",
    "        \n",
    "        #This reshape is what we were missing earlier\n",
    "        out = out.view(-1, 8 * 8 * 8)\n",
    "        \n",
    "        out = self.act3(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a6a43bbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18090, [432, 16, 1152, 8, 16384, 32, 64, 2])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Net()\n",
    "\n",
    "numel_list = [p.numel() for p in model.parameters()]\n",
    "sum(numel_list), numel_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9445cbb",
   "metadata": {},
   "source": [
    "#### Benefits of using functional counterpart for every nn module\n",
    "Functional essentially means having no internal state. Submodules such as nn.Tanh and nn.MaxPool2d do not have parameters and their output value is solely and fully determined by the value input arguments. \n",
    "\n",
    "torch.nn.functional takes inputs and parameters as arguments to the function call, instead of working on the input arguments and stored parameters (like its module counterparts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e70d5775",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(8 * 8 * 8, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
    "        out = out.view(-1, 8 * 8 * 8)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "74e7137f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1049, 0.0675]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Net()\n",
    "model(img.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167e7132",
   "metadata": {},
   "source": [
    "### Training\n",
    "<strong>Function breakdown:</strong>\n",
    "1. Looping over the epochs numbered from 1 to n_epochs. The training loop function takes the number of epocs as an input.\n",
    "2. Loops over the dataset in the batches which were created by the data loader \n",
    "3. Fits a batch of our model\n",
    "4. Computes the loss which we try to minimize\n",
    "5. Getting rid of the gradients from the last iteration in the loop\n",
    "6. Performs the backward step. That is, we compute the gradients of all parameters we want the network to learn.\n",
    "7. Updates the model\n",
    "8. Sums the losses we saw over the epoch. The loss is transformed to a Python number with .item(), to escape the gradients.\n",
    "9. Divides by the length of the training data loader to get the average loss per batch. This is a much more intuitive measure than the sum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6df23f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "    for epoch in range(1, n_epochs + 1):  # <1>\n",
    "        loss_train = 0.0\n",
    "        for imgs, labels in train_loader:  # <2>\n",
    "            \n",
    "            outputs = model(imgs)  # <3>\n",
    "            \n",
    "            loss = loss_fn(outputs, labels)  # <4>\n",
    "\n",
    "            optimizer.zero_grad()  # <5>\n",
    "            \n",
    "            loss.backward()  # <6>\n",
    "            \n",
    "            optimizer.step()  # <7>\n",
    "\n",
    "            loss_train += loss.item()  # <8>\n",
    "\n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            print('{} Epoch {}, Training loss {}'.format(\n",
    "                datetime.datetime.now(), epoch,\n",
    "                loss_train / len(train_loader)))  # <9>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef322ced",
   "metadata": {},
   "source": [
    "<strong>Arguments breakdown:</strong>\n",
    "1. <strong>n_epochs</strong> <br>\n",
    "Number of epochs which we wish to train the model. Looping over the epochs numbered from 1 to n_epochs. \n",
    "2. <strong>optimizer</strong> <br>\n",
    "- An optimizer is a function or an algorithm that modifies the attributes of the neural network, such as weights and learning rate. Thus, it helps in reducing the overall loss and improve the accuracy. In SGD we find out the gradient of the cost function of a single example at each iteration instead of the sum of the gradient of the cost function of all the examples.<br>\n",
    "- Since only one sample from the dataset is chosen at random for each iteration, the path taken by the algorithm to reach the minima is usually noisier than your typical Gradient Descent algorithm <br>\n",
    "- If you look at the pseudocode using the link below, it's clear that theta will be updated until gradient is equal to 0. When the gradient equal to 0, that means that the theta(cost) is at a point where it reached the local (or global ideally, but probably not) minima of the function.<br>\n",
    "(https://www.geeksforgeeks.org/ml-stochastic-gradient-descent-sgd/)\n",
    "3. <strong>model</strong> <br>\n",
    "Instatiate the model and assign it to the training function.\n",
    "4. <strong>loss_fn</strong> <br>\n",
    "Loss functions are used to gauge the error between the prediction output and the provided target value.The word 'loss' means the penalty that the model gets for failing to yield the desired results.<br>\n",
    "(https://towardsdatascience.com/cross-entropy-loss-function-f38c4ec8643e)\n",
    "5. <strong>train_loader</strong> <br>\n",
    "Combines a dataset and a sampler, and provides an iterable (just like a list) over the given dataset.\n",
    "(https://pytorch.org/docs/stable/data.html#:~:text=Data%20loader.,(collation)%20and%20memory%20pinning.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "14d4d81c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-07 01:15:23.900660 Epoch 1, Training loss 0.5886110298952479\n",
      "2022-09-07 01:15:59.427983 Epoch 10, Training loss 0.3407089311605806\n"
     ]
    }
   ],
   "source": [
    "train_loader_shuffled = torch.utils.data.DataLoader(cifar2, batch_size=64,\n",
    "                                           shuffle=True)  # <1>\n",
    "\n",
    "model = Net()  \n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)  \n",
    "loss_fn = nn.CrossEntropyLoss()  \n",
    "\n",
    "training_loop(  # <5>\n",
    "    n_epochs = 10,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader_shuffled,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3674eeb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train: 0.85\n",
      "Accuracy val: 0.84\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64,\n",
    "                                           shuffle=False)\n",
    "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64,\n",
    "                                         shuffle=False)\n",
    "\n",
    "def validate(model, train_loader, val_loader):\n",
    "    for name, loader in [(\"train\", train_loader), (\"val\", val_loader)]:\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():  # <1>\n",
    "            for imgs, labels in loader:\n",
    "                outputs = model(imgs)\n",
    "                _, predicted = torch.max(outputs, dim=1) # <2>\n",
    "                total += labels.shape[0]  # <3>\n",
    "                correct += int((predicted == labels).sum())  # <4>\n",
    "\n",
    "        print(\"Accuracy {}: {:.2f}\".format(name , correct / total))\n",
    "\n",
    "validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bcc836",
   "metadata": {},
   "source": [
    "## Saving & Loading the model\n",
    "The saved .pt file contains all the parameters of model: that is, weights and biases for the two convolution modules and the two linear modules. That means that the weights are being saved and not the structure. When deploying the model into production, we instantiate the model and load the parameters to it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d10fd47c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = 'saved_models/'\n",
    "torch.save(model.state_dict(), data_path + 'sample_model.pt')\n",
    "loaded_model = Net()  # <1>\n",
    "loaded_model.load_state_dict(torch.load(data_path\n",
    "                                        + 'sample_model.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce705a8",
   "metadata": {},
   "source": [
    "## Training on the M1 MacBook built-in GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6d149ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device mps.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('mps') if torch.backends.mps.is_built() else torch.device('cpu') \n",
    "                       \n",
    "print(f\"Training on device {device}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a690850",
   "metadata": {},
   "source": [
    "The training loop is altered by moving the tensors we get from the data loader to the GPU by using the Tensor.to method. Note that the code is exactly like our first version at the beginning of this section except for the two lines moving the inputs to the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c810c7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss_train = 0.0\n",
    "        for imgs, labels in train_loader:\n",
    "            imgs = imgs.to(device=device)  # <1>\n",
    "            labels = labels.to(device=device)\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_train += loss.item()\n",
    "\n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            print('{} Epoch {}, Training loss {}'.format(\n",
    "                datetime.datetime.now(), epoch,\n",
    "                loss_train / len(train_loader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3989d81d",
   "metadata": {},
   "source": [
    "The model is instantiated and moved to the device variable. \n",
    "1. Moves our model (all parameters) to the GPU. If either the model or the inputs are not assigned to the GPU, you will get errors about tensors not being on the same device, because the PyTorch operators do not support mixing GPU and CPU inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "27cea1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64,\n",
    "                                           shuffle=False)\n",
    "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64,\n",
    "                                         shuffle=False)\n",
    "all_acc_dict = collections.OrderedDict()\n",
    "\n",
    "train_loader_shuffled = torch.utils.data.DataLoader(cifar2, batch_size=64,\n",
    "                                           shuffle=True)\n",
    "def validate(model, train_loader, val_loader):\n",
    "    accdict = {}\n",
    "    for name, loader in [(\"train\", train_loader), (\"val\", val_loader)]:\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for imgs, labels in loader:\n",
    "                imgs = imgs.to(device=device)\n",
    "                labels = labels.to(device=device)\n",
    "                outputs = model(imgs)\n",
    "                _, predicted = torch.max(outputs, dim=1) # <1>\n",
    "                total += labels.shape[0]\n",
    "                correct += int((predicted == labels).sum())\n",
    "\n",
    "        print(\"Accuracy {}: {:.2f}\".format(name , correct / total))\n",
    "        accdict[name] = correct / total\n",
    "        accdict['num_parameters'] = sum(p.numel() for p in model.parameters())\n",
    "    return accdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f68a6ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-07 12:53:36.979467 Epoch 1, Training loss 0.587703882129329\n",
      "2022-09-07 12:53:48.505537 Epoch 10, Training loss 0.3339023478091902\n",
      "2022-09-07 12:54:01.365435 Epoch 20, Training loss 0.29509483306271256\n",
      "2022-09-07 12:54:14.114402 Epoch 30, Training loss 0.2697806066483449\n",
      "2022-09-07 12:54:26.872814 Epoch 40, Training loss 0.24916118815256533\n",
      "2022-09-07 12:54:39.660506 Epoch 50, Training loss 0.22985871667694893\n",
      "2022-09-07 12:54:52.425033 Epoch 60, Training loss 0.2134873029437794\n",
      "2022-09-07 12:55:05.254104 Epoch 70, Training loss 0.19640405461856514\n",
      "2022-09-07 12:55:18.022473 Epoch 80, Training loss 0.18283408263306708\n",
      "2022-09-07 12:55:30.755302 Epoch 90, Training loss 0.167821241507105\n",
      "2022-09-07 12:55:43.508378 Epoch 100, Training loss 0.15492085519300144\n",
      "Accuracy train: 0.94\n",
      "Accuracy val: 0.89\n"
     ]
    }
   ],
   "source": [
    "model = Net().to(device=device)  # <1>\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 100,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader_shuffled,\n",
    ")\n",
    "\n",
    "all_acc_dict[\"baseline\"] = validate(model, train_loader, val_loader)\n",
    "torch.save(model.state_dict(), data_path + 'convolutions_baseline.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "45fd11c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train: 0.94\n",
      "Accuracy val: 0.89\n"
     ]
    }
   ],
   "source": [
    "#RUN TO LOAD MODEL\n",
    "model = Net().to(device=device)\n",
    "model.load_state_dict(torch.load(data_path +'convolutions_baseline.pt',map_location=device))\n",
    "all_acc_dict[\"baseline\"] = validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fde338",
   "metadata": {},
   "source": [
    "The <strong>map_location</strong> keyword argument selects the device which the weight will be restored from. PyTorch will attempt to load the weight to the same device it was saved from—that is, weights on the GPU will be restored to the GPU. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36dc0532",
   "metadata": {},
   "source": [
    "## Other model designs\n",
    "### Width\n",
    "The width of the network: the number of neurons per layer, or channels per convolution.\n",
    "The greater the capacity, the more variability in the inputs the model will be able to manage; but at the same time, the more likely overfitting will be, since the model can use a greater number of parameters to memorize unessential aspects of the input. We already went into ways to combat overfitting, the best being increasing the sample size or, in the absence of new data, augmenting existing data through artificial modifica- tions of the same data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c75602ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetWidth(nn.Module):\n",
    "    def __init__(self, n_chans1=32):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3,\n",
    "                               padding=1)\n",
    "        self.fc1 = nn.Linear(8 * 8 * n_chans1 // 2, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
    "        out = out.view(-1, 8 * 8 * self.n_chans1 // 2)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "81673bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-07 12:56:09.261031 Epoch 1, Training loss 0.5632458668985184\n",
      "2022-09-07 12:56:21.911077 Epoch 10, Training loss 0.3106895130910691\n",
      "2022-09-07 12:56:35.982163 Epoch 20, Training loss 0.26706456445204985\n",
      "2022-09-07 12:56:50.064449 Epoch 30, Training loss 0.23679492285676823\n",
      "2022-09-07 12:57:04.065561 Epoch 40, Training loss 0.21174981843703872\n",
      "2022-09-07 12:57:18.042129 Epoch 50, Training loss 0.1882771899935546\n",
      "2022-09-07 12:57:32.036367 Epoch 60, Training loss 0.1619753188862922\n",
      "2022-09-07 12:57:46.060942 Epoch 70, Training loss 0.14169034219471513\n",
      "2022-09-07 12:58:00.104657 Epoch 80, Training loss 0.12131570253497476\n",
      "2022-09-07 12:58:14.102447 Epoch 90, Training loss 0.10476106801515173\n",
      "2022-09-07 12:58:28.117383 Epoch 100, Training loss 0.08651040705620863\n",
      "Accuracy train: 0.96\n",
      "Accuracy val: 0.90\n"
     ]
    }
   ],
   "source": [
    "#RUN TO TRAIN\n",
    "model = NetWidth(n_chans1=32).to(device=device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 100,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader_shuffled,\n",
    ")\n",
    "\n",
    "all_acc_dict[\"width\"] = validate(model, train_loader, val_loader)\n",
    "torch.save(model.state_dict(), data_path + 'convolutions_width.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3b91904c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train: 0.96\n",
      "Accuracy val: 0.90\n"
     ]
    }
   ],
   "source": [
    "#RUN TO LOAD MODEL\n",
    "model = NetWidth().to(device=device)\n",
    "model.load_state_dict(torch.load(data_path +'convolutions_width.pt',map_location=device))\n",
    "all_acc_dict[\"width\"] = validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e169a8",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "A regularization term to the loss. This term is crafted so that the weights of the model tend to be small on their own, limiting how much training makes them grow. In other words, it is a penalty on larger weight values.\n",
    "- <strong>L2 regularization</strong><br>\n",
    "    <strong>a.</strong> The sum of squares of all weights in the model. It is reffered to as weight decay, because the negative gradient of the L2 regularization term with respect to a parameter w_i is - 2 * lambda * w_i.<br>\n",
    "    <strong>b.</strong> Lambda is the aforementioned hyperparameter, simply named weight decay in PyTorch.<br>\n",
    "    <strong>c.</strong> Adding L2 regularization to the loss function is equivalent to decreasing each weight by an amount proportional to its current value during the optimization step (hence, the name weight decay)\n",
    "- <strong>L1 regularization</strong><br>\n",
    "The sum of the absolute values of all weights in the model.\n",
    "\n",
    "\n",
    "Both of them are scaled by a (small) factor called learning rate, which is a hyperparameter we set prior to training. In addition could implement regularization pretty easily by adding a term to the loss. After computing the loss, whatever the loss function is, we can iterate the parameters of the model, sum their respective square (for L2) or abs (for L1), and backpropagate\n",
    "### L2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "37d62d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-07 12:58:32.624929 Epoch 1, Training loss 0.6009550162941028\n",
      "2022-09-07 12:59:04.635168 Epoch 10, Training loss 0.34520352076572974\n",
      "2022-09-07 12:59:39.827888 Epoch 20, Training loss 0.3143721448767717\n",
      "2022-09-07 13:00:14.986861 Epoch 30, Training loss 0.2898828269569737\n",
      "2022-09-07 13:00:49.597912 Epoch 40, Training loss 0.27600450500561174\n",
      "2022-09-07 13:01:24.240430 Epoch 50, Training loss 0.2621211585155718\n",
      "2022-09-07 13:02:00.507855 Epoch 60, Training loss 0.2521908113341423\n",
      "2022-09-07 13:02:35.116370 Epoch 70, Training loss 0.23879054857856907\n",
      "2022-09-07 13:03:09.745909 Epoch 80, Training loss 0.2299113347177293\n",
      "2022-09-07 13:03:44.225656 Epoch 90, Training loss 0.22116255874087096\n",
      "2022-09-07 13:04:18.810896 Epoch 100, Training loss 0.21378838361068897\n",
      "Accuracy train: 0.93\n",
      "Accuracy val: 0.89\n"
     ]
    }
   ],
   "source": [
    "#RUN TO TRAIN\n",
    "def training_loop_l2reg(n_epochs, optimizer, model, loss_fn,\n",
    "                        train_loader):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss_train = 0.0\n",
    "        for imgs, labels in train_loader:\n",
    "            imgs = imgs.to(device=device)\n",
    "            labels = labels.to(device=device)\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            l2_lambda = 0.001\n",
    "            l2_norm = sum(p.pow(2.0).sum()\n",
    "                          for p in model.parameters())  # <1>\n",
    "            loss = loss + l2_lambda * l2_norm\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            loss_train += loss.item()\n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            print('{} Epoch {}, Training loss {}'.format(\n",
    "                datetime.datetime.now(), epoch,\n",
    "                loss_train / len(train_loader)))\n",
    "\n",
    "model = Net().to(device=device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop_l2reg(\n",
    "    n_epochs = 100,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader_shuffled,\n",
    ")\n",
    "\n",
    "torch.save(model.state_dict(), data_path + 'convolutions_l2.pt')\n",
    "all_acc_dict[\"l2 reg\"] = validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4e26935b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train: 0.93\n",
      "Accuracy val: 0.89\n"
     ]
    }
   ],
   "source": [
    "#RUN TO LOAD MODEL\n",
    "model = Net().to(device=device)\n",
    "model.load_state_dict(torch.load(data_path +'convolutions_l2.pt',map_location=device))\n",
    "all_acc_dict[\"l2 reg\"] = validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bc7a70",
   "metadata": {},
   "source": [
    "### L1 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "97cd3c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/egads1/opt/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:173: UserWarning: The operator 'aten::sgn.out' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at  /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/mps/MPSFallback.mm:11.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-07 13:04:23.681773 Epoch 1, Training loss 1.004363123778325\n",
      "2022-09-07 13:04:58.449869 Epoch 10, Training loss 0.6049925026240622\n",
      "2022-09-07 13:05:36.935069 Epoch 20, Training loss 0.4636694899030552\n",
      "2022-09-07 13:06:15.561388 Epoch 30, Training loss 0.414894456316711\n",
      "2022-09-07 13:06:54.189746 Epoch 40, Training loss 0.3960362217228883\n",
      "2022-09-07 13:07:32.562205 Epoch 50, Training loss 0.38106962317114423\n",
      "2022-09-07 13:08:11.057329 Epoch 60, Training loss 0.36975727453353296\n",
      "2022-09-07 13:08:49.647207 Epoch 70, Training loss 0.36330467604907457\n",
      "2022-09-07 13:09:28.104502 Epoch 80, Training loss 0.35711024151106546\n",
      "2022-09-07 13:10:06.582170 Epoch 90, Training loss 0.3511013913496285\n",
      "2022-09-07 13:10:45.201962 Epoch 100, Training loss 0.34266434211260194\n",
      "Accuracy train: 0.88\n",
      "Accuracy val: 0.88\n"
     ]
    }
   ],
   "source": [
    "def training_loop_l1reg(n_epochs, optimizer, model, loss_fn,\n",
    "                        train_loader):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss_train = 0.0\n",
    "        for imgs, labels in train_loader:\n",
    "            imgs = imgs.to(device=device)\n",
    "            labels = labels.to(device=device)\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            l1_lambda = 0.001\n",
    "            l1_norm = sum(abs(p).sum()\n",
    "                          for p in model.parameters())  # <1>\n",
    "            loss = loss + l1_lambda * l1_norm\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            loss_train += loss.item()\n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            print('{} Epoch {}, Training loss {}'.format(\n",
    "                datetime.datetime.now(), epoch,\n",
    "                loss_train / len(train_loader)))\n",
    "            \n",
    "model = Net().to(device=device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop_l1reg(\n",
    "    n_epochs = 100,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader_shuffled,\n",
    ")\n",
    "\n",
    "torch.save(model.state_dict(), data_path + 'convolutions_l1.pt')\n",
    "all_acc_dict[\"l1 reg\"] = validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "953acf62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train: 0.88\n",
      "Accuracy val: 0.88\n"
     ]
    }
   ],
   "source": [
    "#RUN TO LOAD MODEL\n",
    "model = Net().to(device=device)\n",
    "model.load_state_dict(torch.load(data_path +'convolutions_l1.pt',map_location=device))\n",
    "all_acc_dict[\"l1 reg\"] = validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbac4cc8",
   "metadata": {},
   "source": [
    "### Dropout\n",
    "The idea behind dropout is indeed simple: zero out a random fraction of out- puts from neurons across the network, where the randomization happens at each training iteration. We can implement dropout in a model by adding an nn.Dropout mod- ule between the nonlinear activation function and the linear or convolutional module of the subsequent layer.\n",
    "\n",
    "Note that dropout is normally active during training, while during the evaluation of a trained model in production, dropout is bypassed or, equivalently, assigned a proba- bility equal to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9ec344d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetDropout(nn.Module):\n",
    "    def __init__(self, n_chans1=32):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv1_dropout = nn.Dropout2d(p=0.4)\n",
    "        self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3,\n",
    "                               padding=1)\n",
    "        self.conv2_dropout = nn.Dropout2d(p=0.4)\n",
    "        self.fc1 = nn.Linear(8 * 8 * n_chans1 // 2, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
    "        out = self.conv1_dropout(out)\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
    "        out = self.conv2_dropout(out)\n",
    "        out = out.view(-1, 8 * 8 * self.n_chans1 // 2)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "fbcfa828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-07 13:10:48.674329 Epoch 1, Training loss 0.5898326441740535\n",
      "2022-09-07 13:11:09.468448 Epoch 10, Training loss 0.3786069510658835\n",
      "2022-09-07 13:11:32.554726 Epoch 20, Training loss 0.34664218241621736\n",
      "2022-09-07 13:11:55.845517 Epoch 30, Training loss 0.3290299878568406\n",
      "2022-09-07 13:28:19.891772 Epoch 40, Training loss 0.31337567252717957\n",
      "2022-09-07 13:28:43.233751 Epoch 50, Training loss 0.2969393600134333\n",
      "2022-09-07 13:29:06.266754 Epoch 60, Training loss 0.28485933022134624\n",
      "2022-09-07 13:29:29.414941 Epoch 70, Training loss 0.27454671880621817\n",
      "2022-09-07 13:46:56.850964 Epoch 80, Training loss 0.2643285835055029\n",
      "2022-09-07 13:47:19.993556 Epoch 90, Training loss 0.2521951465280193\n",
      "2022-09-07 13:47:43.057751 Epoch 100, Training loss 0.24275666309200275\n",
      "Accuracy train: 0.90\n",
      "Accuracy val: 0.87\n"
     ]
    }
   ],
   "source": [
    "#RUN TO TRAIN MODEL\n",
    "model = NetDropout(n_chans1=32).to(device=device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 100,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader_shuffled,\n",
    ")\n",
    "torch.save(model.state_dict(), data_path + 'convolutions_dropout.pt')\n",
    "all_acc_dict[\"dropout\"] = validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8090bebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train: 0.89\n",
      "Accuracy val: 0.87\n"
     ]
    }
   ],
   "source": [
    "#RUN TO LOAD MODEL\n",
    "model = NetDropout().to(device=device)\n",
    "model.load_state_dict(torch.load(data_path +'convolutions_dropout.pt',map_location=device))\n",
    "all_acc_dict[\"l1 reg\"] = validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d72cdd",
   "metadata": {},
   "source": [
    "### Batch Normalization\n",
    "The main idea behind batch normalization is to rescale the inputs to the activa- tions of the network so that minibatches have a certain desirable distribution. Recall- ing the mechanics of learning and the role of nonlinear activation functions, this helps avoid the inputs to activation functions being too far into the saturated portion of the function, thereby killing gradients and slowing training.\n",
    "\n",
    "\n",
    "In practical terms, batch normalization shifts and scales an intermediate input using the mean and standard deviation collected at that intermediate location over the samples of the minibatch. The regularization effect is a result of the fact that an individual sample and its downstream activations are always seen by the model as shifted and scaled, depending on the statistics across the randomly extracted mini- batch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "aa7d4cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetBatchNorm(nn.Module):\n",
    "    def __init__(self, n_chans1=32):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv1_batchnorm = nn.BatchNorm2d(num_features=n_chans1)\n",
    "        self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3, \n",
    "                               padding=1)\n",
    "        self.conv2_batchnorm = nn.BatchNorm2d(num_features=n_chans1 // 2)\n",
    "        self.fc1 = nn.Linear(8 * 8 * n_chans1 // 2, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.conv1_batchnorm(self.conv1(x))\n",
    "        out = F.max_pool2d(torch.tanh(out), 2)\n",
    "        out = self.conv2_batchnorm(self.conv2(out))\n",
    "        out = F.max_pool2d(torch.tanh(out), 2)\n",
    "        out = out.view(-1, 8 * 8 * self.n_chans1 // 2)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "74686aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-07 15:46:52.943392 Epoch 1, Training loss 0.4705844859408725\n",
      "2022-09-07 15:47:10.285528 Epoch 10, Training loss 0.27317707193125584\n",
      "2022-09-07 15:47:29.493500 Epoch 20, Training loss 0.21526111690861405\n",
      "2022-09-07 15:47:48.895007 Epoch 30, Training loss 0.1691231123486142\n",
      "2022-09-07 15:48:08.455025 Epoch 40, Training loss 0.12914200302711717\n",
      "2022-09-07 15:48:27.655657 Epoch 50, Training loss 0.09329307033045656\n",
      "2022-09-07 15:48:46.787668 Epoch 60, Training loss 0.06438476900765851\n",
      "2022-09-07 15:49:05.949686 Epoch 70, Training loss 0.043585976099322556\n",
      "2022-09-07 15:49:25.299337 Epoch 80, Training loss 0.03006327949511777\n",
      "2022-09-07 15:49:44.328313 Epoch 90, Training loss 0.018924324304624728\n",
      "2022-09-07 15:50:03.436691 Epoch 100, Training loss 0.013051004848994647\n",
      "Accuracy train: 0.99\n",
      "Accuracy val: 0.89\n"
     ]
    }
   ],
   "source": [
    "#TRAIN MODEL\n",
    "model = NetBatchNorm(n_chans1=32).to(device=device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 100,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader_shuffled,\n",
    ")\n",
    "torch.save(model.state_dict(), data_path + 'convolutions_batchnorm.pt')\n",
    "all_acc_dict[\"batch_norm\"] = validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0da0b419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train: 0.99\n",
      "Accuracy val: 0.89\n"
     ]
    }
   ],
   "source": [
    "#RUN TO LOAD MODEL\n",
    "model = NetBatchNorm().to(device=device)\n",
    "model.load_state_dict(torch.load(data_path +'convolutions_batchnorm.pt',map_location=device))\n",
    "all_acc_dict[\"batch_norm\"] = validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f90b130",
   "metadata": {},
   "source": [
    "### Very deep model\n",
    "With depth, the complexity of the function the network is able to approximate generally increases.\n",
    "The bottom line is that a long chain of multiplications will tend to make the contribu- tion of the parameter to the gradient vanish, leading to ineffective training of that layer since that parameter and others like it won’t be properly updated.\n",
    "\n",
    "\n",
    "A skip connection is nothing but the addition of the input to the output of a block of layers. This is exactly how it is done in PyTorch. Let’s add one layer to our simple convolutional model, and let’s use ReLU as the acti- vation for a change. The vanilla mod- ule with an extra layer looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "73f5eb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetDepth(nn.Module):\n",
    "    def __init__(self, n_chans1=32):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3,\n",
    "                               padding=1)\n",
    "        self.conv3 = nn.Conv2d(n_chans1 // 2, n_chans1 // 2,\n",
    "                               kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(4 * 4 * n_chans1 // 2, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.relu(self.conv2(out)), 2)\n",
    "        out = F.max_pool2d(torch.relu(self.conv3(out)), 2)\n",
    "        out = out.view(-1, 4 * 4 * self.n_chans1 // 2)\n",
    "        out = torch.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "cfaf5e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-10 00:25:02.018020 Epoch 1, Training loss 0.6702603185252779\n",
      "2022-09-10 00:25:17.212488 Epoch 10, Training loss 0.35272856749546755\n",
      "2022-09-10 00:25:33.986158 Epoch 20, Training loss 0.30694997424532655\n",
      "2022-09-10 00:25:50.755099 Epoch 30, Training loss 0.27966196085237394\n",
      "2022-09-10 00:26:07.428161 Epoch 40, Training loss 0.24270865496746294\n",
      "2022-09-10 00:26:24.144358 Epoch 50, Training loss 0.21667908862897545\n",
      "2022-09-10 00:26:40.695431 Epoch 60, Training loss 0.1906617257008507\n",
      "2022-09-10 00:26:57.275284 Epoch 70, Training loss 0.16516237506631073\n",
      "2022-09-10 00:27:14.031089 Epoch 80, Training loss 0.1409378789697483\n",
      "2022-09-10 00:27:30.679200 Epoch 90, Training loss 0.12246800209306608\n",
      "2022-09-10 00:27:47.424028 Epoch 100, Training loss 0.10901181698794578\n",
      "Accuracy train: 0.97\n",
      "Accuracy val: 0.91\n"
     ]
    }
   ],
   "source": [
    "model = NetDepth(n_chans1=32).to(device=device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 100,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader_shuffled,\n",
    ")\n",
    "all_acc_dict[\"depth\"] = validate(model, train_loader, val_loader)\n",
    "torch.save(model.state_dict(), data_path + 'convolutions_depth.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "44850045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train: 0.97\n",
      "Accuracy val: 0.91\n"
     ]
    }
   ],
   "source": [
    "#RUN TO LOAD MODEL\n",
    "model = NetDepth(n_chans1=32).to(device=device)\n",
    "model.load_state_dict(torch.load(data_path +'convolutions_depth.pt',map_location=device))\n",
    "all_acc_dict[\"depth\"] = validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478bce90",
   "metadata": {},
   "source": [
    "Thinking about backpropagation, we can appreciate that a skip connection, or a sequence of skip connections in a deep network, creates a direct path from the deeper parameters to the loss. This makes their contribution to the gradient of the loss more direct, as partial derivatives of the loss with respect to those parameters have a chance not to be multiplied by a long chain of other operations.\n",
    "\n",
    "\n",
    "Adding a skip connection a la ResNet to this model amounts to adding the output of the first layer in the forward function to the input of the third layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "b1dfac2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetRes(nn.Module):\n",
    "    def __init__(self, n_chans1=32):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3,\n",
    "                               padding=1)\n",
    "        self.conv3 = nn.Conv2d(n_chans1 // 2, n_chans1 // 2,\n",
    "                               kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(4 * 4 * n_chans1 // 2, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.relu(self.conv2(out)), 2)\n",
    "        out1 = out\n",
    "        out = F.max_pool2d(torch.relu(self.conv3(out)) + out1, 2)\n",
    "        out = out.view(-1, 4 * 4 * self.n_chans1 // 2)\n",
    "        out = torch.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "8e0dde51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-10 00:32:11.264815 Epoch 1, Training loss 0.6393668503518317\n",
      "2022-09-10 00:32:26.251494 Epoch 10, Training loss 0.32598624535047327\n",
      "2022-09-10 00:32:42.814641 Epoch 20, Training loss 0.2794313859787716\n",
      "2022-09-10 00:32:59.374146 Epoch 30, Training loss 0.2423049727822565\n",
      "2022-09-10 00:33:15.992187 Epoch 40, Training loss 0.2123127522741913\n",
      "2022-09-10 00:33:32.528696 Epoch 50, Training loss 0.1845850465688736\n",
      "2022-09-10 00:33:49.120838 Epoch 60, Training loss 0.15926173243932662\n",
      "2022-09-10 00:34:05.851112 Epoch 70, Training loss 0.13583020711685442\n",
      "2022-09-10 00:34:22.372535 Epoch 80, Training loss 0.11333369243012112\n",
      "2022-09-10 00:34:38.907773 Epoch 90, Training loss 0.09544831299620449\n",
      "2022-09-10 00:34:55.496732 Epoch 100, Training loss 0.07482479226769535\n",
      "Accuracy train: 0.95\n",
      "Accuracy val: 0.89\n"
     ]
    }
   ],
   "source": [
    "model = NetRes(n_chans1=32).to(device=device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 100,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")\n",
    "all_acc_dict[\"res\"] = validate(model, train_loader, val_loader)\n",
    "torch.save(model.state_dict(), data_path + 'convolutions_res.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "70815d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train: 0.95\n",
      "Accuracy val: 0.89\n"
     ]
    }
   ],
   "source": [
    "#RUN TO LOAD MODEL\n",
    "model = NetRes(n_chans1=32).to(device=device)\n",
    "model.load_state_dict(torch.load(data_path +'convolutions_res.pt',map_location=device))\n",
    "all_acc_dict[\"depth\"] = validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0ad375",
   "metadata": {},
   "source": [
    "### Very deep model; +100 layers in a CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1998596e",
   "metadata": {},
   "source": [
    "This is a module subclass whose sole job is to provide the computation for one block that is, one group of convolutions, activation, and skip connection:\n",
    "\n",
    "\n",
    "1. The BatchNorm layer would cancel the effect of bias, so it is customarily left out. The batch normalization in the block, since this will help prevent gradients from vanishing during training.\n",
    "2. Uses custom initializations. kaiming_normal_ initializes with normal random elements with standard deviation as computed in the ResNet paper. The batch norm is initialized to produce output distributions that initially have 0 mean and 0.5 variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "5aebffbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, n_chans):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(n_chans, n_chans, kernel_size=3,\n",
    "                              padding=1, bias=False)  # <1>\n",
    "        self.batch_norm = nn.BatchNorm2d(num_features=n_chans)\n",
    "        torch.nn.init.kaiming_normal_(self.conv.weight,\n",
    "                                      nonlinearity='relu')  # <2>\n",
    "        torch.nn.init.constant_(self.batch_norm.weight, 0.5)\n",
    "        torch.nn.init.zeros_(self.batch_norm.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = self.batch_norm(out)\n",
    "        out = torch.relu(out)\n",
    "        return out + x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36699048",
   "metadata": {},
   "source": [
    "First, in init, we create nn.Sequential containing a list of ResBlock instances. nn.Sequential will ensure that the output of one block is used as input to the next. It will also ensure that all the parameters in the block are visible to Net. Then, in forward, we just call the sequential to traverse the 100 blocks and generate the output:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "5a8fc686",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetResDeep(nn.Module):\n",
    "    def __init__(self, n_chans1=32, n_blocks=10):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.resblocks = nn.Sequential(\n",
    "            *(n_blocks * [ResBlock(n_chans=n_chans1)]))\n",
    "        self.fc1 = nn.Linear(8 * 8 * n_chans1, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
    "        out = self.resblocks(out)\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = out.view(-1, 8 * 8 * self.n_chans1)\n",
    "        out = torch.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "b96f2b15",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [117]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mSGD(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3e-3\u001b[39m)\n\u001b[1;32m      3\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m----> 5\u001b[0m \u001b[43mtraining_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_loader_shuffled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m all_acc_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mres deep\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m validate(model, train_loader, val_loader)\n",
      "Input \u001b[0;32mIn [68]\u001b[0m, in \u001b[0;36mtraining_loop\u001b[0;34m(n_epochs, optimizer, model, loss_fn, train_loader)\u001b[0m\n\u001b[1;32m      8\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(outputs, labels)\n\u001b[1;32m     10\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 11\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     14\u001b[0m loss_train \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = NetResDeep(n_chans1=32, n_blocks=100).to(device=device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=3e-3)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 1,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader_shuffled,\n",
    ")\n",
    "all_acc_dict[\"res deep\"] = validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a256312",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUN TO LOAD MODEL\n",
    "model = NetResDeep(n_chans1=32).to(device=device)\n",
    "model.load_state_dict(torch.load(data_path +'convolutions_resdeep.pt',map_location=device))\n",
    "all_acc_dict[\"resdeep\"] = validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "9c35a89f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "trn_acc = [v['train'] for k, v in all_acc_dict.items()]\n",
    "val_acc = [v['val'] for k, v in all_acc_dict.items()]\n",
    "\n",
    "width =0.3\n",
    "plt.bar(np.arange(len(trn_acc)), trn_acc, width=width, label='train')\n",
    "plt.bar(np.arange(len(val_acc))+ width, val_acc, width=width, label='val')\n",
    "plt.xticks(np.arange(len(val_acc))+ width/2, list(all_acc_dict.keys()),\n",
    "           rotation=60)\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylim(0.7, 1)\n",
    "plt.savefig('accuracy_comparison.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
